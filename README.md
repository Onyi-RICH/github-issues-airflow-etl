# GitHub Issues ETL Pipeline (Apache Airflow)

ğŸš€ **Production-grade data engineering pipeline built with Apache Airflow, Docker, and PostgreSQL**

This project demonstrates how to design, debug, and operate a real-world ETL system that ingests GitHub repository and issue event data incrementally and reliably.

**Key Skills Demonstrated**
- Apache Airflow DAG design
- Incremental & idempotent ETL
- Dockerized orchestration
- API data modeling
- Production debugging & observability
- Cloud-ready architecture (MWAA / Kubernetes)

ğŸ”— **Tech Stack:** Python Â· Airflow Â· Docker Â· PostgreSQL Â· GitHub API

---

## TL;DR â€“ How to Run This Project

```bash
git clone https://github.com/Onyi-RICH/github-issues-airflow-etl
cd github-issues-airflow-etl
docker compose up --build

---
## Table of Contents

1. [Project Overview](#1-project-overview)
2. [Mental Model ğŸ§ ](#2-mental-model-)
3. [Architecture Overview](#3-architecture-overview)
4. [File Structure](#4-file-structure)
5. [Core Concepts & Acronyms (Studentâ€‘Friendly)](#5-core-concepts--acronyms-studentfriendly)
6. [Airflow DAG Design (OOPâ€‘Style)](#6-airflow-dag-design-oopstyle)
7. [Incremental Issue & Repository Extraction Strategy](#7-incremental-issue--repository-extraction-strategy)
8. [Authentication & Secrets](#8-authentication--secrets-)
9. [Database Schema & Deliverables](#9-database-schema--deliverables)
10. [Indexing Strategy (Bâ€‘Tree)](#10-indexing-strategy-btree)
11. [Data Cleaning Rules](#11-data-cleaning-rules)
12. [Observability, SLA & Alerts](#12-observability-sla--alerts)
13. [Docker â†” Airflow Execution Guide](#13-docker--airflow-execution-guide)
14. [Kubernetes / MWAA Readiness](#14-kubernetes--mwaa-readiness)
15. [Challenges & Solutions](#15-challenges--solutions)
16. [Practical Tips](#16-practical-tips)
17. [Recommendations](#17-recommendations)
18. [Summary](#18-summary)

---

## 1. Project Overview

### Task

**Wrap GitHub Issues Extraction Script Into an Airflow DAG**

### Description

A Python ETL pipeline extracts **GitHub repositories and issueâ€‘level event data** (issues, comments, timeline events), transforms it into a **rowâ€‘perâ€‘event** analytical table, and loads it into PostgreSQL. The workflow is orchestrated endâ€‘toâ€‘end by **Apache Airflow running in Docker**.

### Objectives

* Automate GitHub data ingestion on a schedule
* No hardâ€‘coded secrets (tokens rotate!)
* Idempotent & safe reâ€‘runs
* Clear task separation & reproducibility
* Productionâ€‘ready Airflow design
* Cloudâ€‘ready (Kubernetes / AWS MWAA)

### Who This Project Is For

This project is designed for:
- Data Engineers learning **real Apache Airflow**
- Analytics Engineers consuming GitHub operational data
- Teams migrating pipelines to **Docker, Kubernetes, or AWS MWAA**
- Interviewers evaluating production-level ETL design

It intentionally models real-world failure scenarios and recovery strategies.

---

## 2. Mental Model ğŸ§ 

```
GitHub API
   â†“
Incremental Repo Sync
   â†“
Incremental Issue Extraction
   â†“
Normalize & Clean (Python)
   â†“
Idempotent Load (Postgres)
   â†“
Watermark Update
   â†“
Airflow Dataset â†’ Downstream DAGs
```

**Key idea:**
ğŸ‘‰ *Stateless Airflow tasks + Stateful database*

* Airflow tasks can fail, retry, or restart safely
* The database is the single source of truth

---

## 3. Architecture Overview

**Components**

* **GitHub API** â€“ Source system
* **Apache Airflow** â€“ Orchestration layer
* **PostgreSQL (Business DB)** â€“ Repositories & issues
* **Airflow Metadata DB** â€“ DAG state & scheduling
* **Docker** â€“ Reproducible runtime

**Why Metadata DB â‰  Business DB?**

* Airflow metadata is operational
* Business data is analytical
* Separating them prevents corruption & scaling issues

---

## 4. File Structure

```
github-issues-airflow-etl/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ github_issues_etl_dag.py
â”œâ”€â”€ github_etl/
â”‚   â”œâ”€â”€ extract_all_issues.py        # Issue + timeline extraction
â”‚   â”œâ”€â”€ extract_repositories.py      # Repository metadata extraction
â”‚   â”œâ”€â”€ load_to_postgres.py          # Issue event loader (idempotent)
â”‚   â”œâ”€â”€ load_repositories.py         # Repository loader (FK parent)
â”‚   â”œâ”€â”€ utils.py                     # Caching & cleaning helpers
|   â”œâ”€â”€ config.py                     # environment variables like DATABASE_URL, GITHUB_PAT
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ test_db_load.py                  # Issue ETL validation
â”œâ”€â”€ test_repo_load.py                # Repository ETL validation
â””â”€â”€ DAG_README.md                    # Full project documentation
```

---

## 5. Core Concepts & Acronyms (Studentâ€‘Friendly)

* **DAG** â€“ Directed Acyclic Graph (task dependency graph)
* **API** â€“ Application Programming Interface (GitHub endpoint)
* **OOPâ€‘Style DAG** â€“ Tasks grouped logically, reusable, testable
* **Bâ€‘Tree Index** â€“ Fast lookup structure used by PostgreSQL
* **SLA** â€“ Service Level Agreement (max allowed runtime)
* **XComâ€‘safe payloads** â€“ Small metadata only, not raw dataframes
* **CeleryExecutor** â€“ Distributed Airflow workers (parallel tasks)
* **Docker** â€“ Package app + dependencies consistently
* **Kubernetes** â€“ Orchestrates containers at scale
* **AWS MWAA** â€“ Managed Apache Airflow on AWS

---

## 6. Airflow DAG Design (OOPâ€‘Style)

### DAG: `github_issues_etl`

**Four Logical Tasks**

1. **Sync Repositories** â€“ Incremental repo metadata load
2. **Get Watermark** â€“ Last successful issue timestamp
3. **Extract & Load Issues** â€“ Incremental issue ingestion
4. **Update Watermark + Dataset Emit** â€“ Commit success

This DAG was intentionally kept to four tasks to maximize observability, reduce retry complexity, and make failures immediately diagnosable.
---

## 7. Incremental Issue & Repository Extraction Strategy

* Repositories synced first (upsert by `repo_id`)
* Issues fetched **only if updated since last run**
* Timeline & comments extracted per issue
* Database enforces uniqueness via `detail_node_id`

**Benefits**

* Faster DAGs
* GitHub rateâ€‘limit safe
* Reâ€‘runs are safe

---

## 8. Authentication & Secrets ğŸ”

**Never hardcode tokens**

Secrets are stored in **Airflow Connections**:

* `github_conn` â†’ GitHub PAT
* `neon_db` â†’ PostgreSQL

Works the same in:

* Docker
* Kubernetes
* AWS MWAA

---

## 9. Database Schema & Deliverables

### Table: `github_source_data.real_github_issues`

| Column           | Type        | Key | Description                 | Example              |
| ---------------- | ----------- | --- | --------------------------- | -------------------- |
| detail_node_id   | TEXT        | PK  | Global GitHub event ID      | I_kwDOâ€¦              |
| repo_id          | TEXT        | FK  | Repository ID               | 1044093046           |
| issue_id         | INT         |     | Issue number                | 26                   |
| issue_title      | TEXT        |     | Issue title                 | Wrap DAG             |
| source           | TEXT        |     | issue / comments / timeline | timeline             |
| timestamp        | TIMESTAMPTZ |     | Event time                  | 2025â€‘12â€‘05           |
| actor            | TEXT        |     | User who triggered          | apiterskaia          |
| action           | TEXT        |     | Event action                | assigned             |
| detail_id        | TEXT        |     | GitHub event ID             | 2138570              |
| assignee         | TEXT        |     | Assigned user               | Onyiâ€‘RICH            |
| current_assignee | TEXT        |     | Current owner               | Onyiâ€‘RICH            |
| repo_name        | TEXT        |     | Repo name                   | github_task_analyzer |
| owner            | TEXT        |     | Repo owner                  | apiterskaia          |

---

## 10. Indexing Strategy (Bâ€‘Tree)

```sql
CREATE UNIQUE INDEX idx_detail_node_id
ON github_source_data.real_github_issues(detail_node_id);
```

**Why Bâ€‘Tree?**

* Fast equality checks
* Efficient range scans
* Default & battleâ€‘tested

---

## 11. Data Cleaning Rules

The pipeline applies **defensive, schema-aligned cleaning** to guarantee Airflow safety.

Rules enforced:

* Missing columns are auto-created (prevents Airflow XCom shape errors)
* Null assignees â†’ `"N/A"`
* `detail_id` stored as `TEXT` (prevents bigint overflow from GitHub IDs)
* Invalid GitHub IDs containing `/` are nulled
* NumPy scalar types converted to native Python types
* Timestamps coerced safely to `TIMESTAMPTZ`
* In-memory deduplication + database uniqueness enforcement

These rules ensure the pipeline remains stable even when GitHub payloads evolve or arrive partially.


---

## 12. Observability, SLA & Alerts

Built-in observability ensures failures are visible and recoverable.

* SLA: 20 minutes per DAG run
* Retries: 2
* Structured logging per task
* Explicit failure callbacks
* Airflow Dataset emitted on successful load

Operational guarantees:
* Failed tasks do NOT advance watermarks
* Partial loads never corrupt downstream tables
* Logs remain readable across Docker containers

---

## 13. Docker â†” Airflow Execution Guide

**Cold start (recommended)**

```powershell
docker compose down -v --Clean everything
docker compose up --build/Start Airflow services 
```

Then:

1. Open [http://localhost:8080](http://localhost:8080)
2. Login (airflow / airflow)
3. Trigger `github_issues_etl`

---

## 14. Kubernetes / MWAA Readiness

This project is already compatible with:

* KubernetesExecutor
* CeleryExecutor
* AWS MWAA

Why?

* No local state
* Externalized secrets
* Stateless tasks

---

## 15. Challenges & Solutions

| Challenge | Root Cause | Solution |
|--------|-----------|---------|
| `bigint out of range` | GitHub IDs exceed Postgres bigint | Cast all GitHub IDs to `TEXT` |
| Foreign key violations | Issues loaded before repos | Enforced repo sync first |
| `numpy.int64` psycopg2 errors | Pandas scalar incompatibility | Normalize to native Python types |
| Missing columns in Airflow | XCom â†’ DataFrame shape loss | Defensive column creation |
| Broken DAG (Python typing) | Airflow runs Python 3.8 | Avoid `datetime | None` syntax |
| Log access `403 Forbidden` | Mismatched Airflow secret keys | Unified `AIRFLOW__WEBSERVER__SECRET_KEY` |
| Watermark table missing | No bootstrap metadata | Explicit metadata schema creation |
| Duplicate inserts | Task retries | `ON CONFLICT DO NOTHING` |
| Slow full reloads | No incrementality | Incremental GitHub API queries |

---

## 16. Practical Tips

* Never trust upstream APIs â€” trust your database
* Assume every task will retry
* Always design for partial failure
* Enforce schema defensively
* Keep Airflow stateless
* Put state (watermarks, keys) in the database
* Docker issues often masquerade as Airflow issues

---

## 17. Recommendations

Next steps:

* Dynamic task mapping per repo
* OpenLineage integration
* Data quality checks
* CI for DAG parsing

---

## 18. Summary

This project delivers a **productionâ€‘grade GitHub Issues ETL** that is:

* Incremental
* Idempotent
* Observable
* Secure
* Dockerâ€‘first
* Cloudâ€‘ready

It demonstrates not just how to build pipelines, but how to operate them safely in production.

---

## ğŸ“¸ Pipeline in Action

![Airflow DAG](screenshots/dag_graph.png)
![Successful Run](screenshots/dag_success.png)

---

## License

This project is licensed under the MIT License.